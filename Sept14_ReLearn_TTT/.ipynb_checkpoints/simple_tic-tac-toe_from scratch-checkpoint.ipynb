{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "  def __init__(self, eps=0.1, alpha=0.5):\n",
    "    self.eps = eps # probability of choosing random action instead of greedy\n",
    "    self.alpha = alpha # learning rate\n",
    "    self.verbose = False\n",
    "    self.state_history = []\n",
    "  \n",
    "  def setV(self, V):\n",
    "    self.V = V\n",
    "\n",
    "  def set_symbol(self, sym):\n",
    "    self.sym = sym\n",
    "\n",
    "  def set_verbose(self, v):\n",
    "    # if true, will print values for each position on the board\n",
    "    self.verbose = v\n",
    "\n",
    "  def reset_history(self):\n",
    "    self.state_history = []\n",
    "\n",
    "  def take_action(self, env):\n",
    "    # choose an action based on epsilon-greedy strategy\n",
    "    r = np.random.rand()\n",
    "    best_state = None\n",
    "    if r < self.eps:\n",
    "      # take a random action\n",
    "      if self.verbose:\n",
    "        print(\"Taking a random action\")\n",
    "\n",
    "      possible_moves = []\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            possible_moves.append((i, j))\n",
    "      idx = np.random.choice(len(possible_moves))\n",
    "      next_move = possible_moves[idx]\n",
    "    else:\n",
    "      # choose the best action based on current values of states\n",
    "      # loop through all possible moves, get their values\n",
    "      # keep track of the best value\n",
    "      pos2value = {} # for debugging\n",
    "      next_move = None\n",
    "      best_value = -1\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            # what is the state if we made this move?\n",
    "            env.board[i,j] = self.sym\n",
    "            state = env.get_state()\n",
    "            env.board[i,j] = 0 # don't forget to change it back!\n",
    "            pos2value[(i,j)] = self.V[state]\n",
    "            if self.V[state] > best_value:\n",
    "              best_value = self.V[state]\n",
    "              best_state = state\n",
    "              next_move = (i, j)\n",
    "\n",
    "      # if verbose, draw the board w/ the values\n",
    "      if self.verbose:\n",
    "        print(\"Taking a greedy action\")\n",
    "        for i in range(LENGTH):\n",
    "          print(\"------------------\")\n",
    "          for j in range(LENGTH):\n",
    "            if env.is_empty(i, j):\n",
    "              # print the value\n",
    "              print(\" %.2f|\" % pos2value[(i,j)], end=\"\")\n",
    "            else:\n",
    "              print(\"  \", end=\"\")\n",
    "              if env.board[i,j] == env.x:\n",
    "                print(\"x  |\", end=\"\")\n",
    "              elif env.board[i,j] == env.o:\n",
    "                print(\"o  |\", end=\"\")\n",
    "              else:\n",
    "                print(\"   |\", end=\"\")\n",
    "          print(\"\")\n",
    "        print(\"------------------\")\n",
    "\n",
    "    # make the move\n",
    "    env.board[next_move[0], next_move[1]] = self.sym\n",
    "\n",
    "  def update_state_history(self, s):\n",
    "    # cannot put this in take_action, because take_action only happens\n",
    "    # once every other iteration for each player\n",
    "    # state history needs to be updated every iteration\n",
    "    # s = env.get_state() # don't want to do this twice so pass it in\n",
    "    self.state_history.append(s)\n",
    "\n",
    "  def update(self, env):\n",
    "    # we want to BACKTRACK over the states, so that:\n",
    "    # V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state))\n",
    "    # where V(next_state) = reward if it's the most current state\n",
    "    #\n",
    "    # NOTE: we ONLY do this at the end of an episode\n",
    "    # not so for all the algorithms we will study\n",
    "    reward = env.reward(self.sym)\n",
    "    target = reward\n",
    "    for prev in reversed(self.state_history):\n",
    "      value = self.V[prev] + self.alpha*(target - self.V[prev])\n",
    "      self.V[prev] = value\n",
    "      target = value\n",
    "    self.reset_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
