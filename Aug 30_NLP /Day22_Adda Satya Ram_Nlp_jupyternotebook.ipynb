{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import math as m\n",
    "import xlrd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path + '/War_And_Peace.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'War', 'and', 'Peace,', 'by', 'Leo', 'Tolstoy', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.']\n"
     ]
    }
   ],
   "source": [
    "w = f.split()\n",
    "print(w[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But I warn you, if you don’t tell me that this means war,\n",
      "if you still try to defend the infamies and horrors perpetrated by that\n",
      "Antichrist—I really believe he is Antichrist—I will have nothing\n",
      "more to do with you and you are no longer my friend, no longer my\n",
      "‘faithful slave,’ as you call yourself!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26547"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(f)\n",
    "print(sentences[2])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 'NNP'),\n",
       " ('u', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " (' ', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " (' ', 'VBP'),\n",
       " ('w', 'WRB'),\n",
       " ('a', 'DT'),\n",
       " ('r', 'NN'),\n",
       " ('n', 'JJ')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### POS Tagging  ---- Wrong Way -----\n",
    "tagged = nltk.pos_tag(sentences[2])\n",
    "tagged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'War', 'and', 'Peace', ',', 'by', 'Leo', 'Tolstoy', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "674585"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "words = word_tokenize(f)\n",
    "print(words[:34])\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\ufeff', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('Project', 'NNP'),\n",
       " ('Gutenberg', 'NNP'),\n",
       " ('EBook', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('War', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Peace', 'NNP'),\n",
       " (',', ','),\n",
       " ('by', 'IN'),\n",
       " ('Leo', 'NNP'),\n",
       " ('Tolstoy', 'NNP'),\n",
       " ('This', 'DT'),\n",
       " ('eBook', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('use', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### POS Tagging\n",
    "tagged = nltk.pos_tag(words[:20])\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'War', 'and', 'Peace', 'by', 'Leo', 'Tolstoy', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'Title', 'War', 'and', 'Peace', 'Author', 'Leo', 'Tolstoy', 'Translators', 'Louise', 'and', 'Aylmer', 'Maude', 'Posting', 'Date', 'January', 'EBook', 'Last', 'Updated', 'December', 'Language', 'English', 'Character', 'set', 'encoding', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'WAR', 'AND', 'PEACE', 'An', 'Anonymous', 'Volunteer', 'and', 'David', 'Widger', 'WAR', 'AND', 'PEACE', 'By', 'Leo', 'CONTENTS', 'BOOK', 'ONE']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "567054"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all tokens that are not alphabetic\n",
    "only_words = [word for word in words if word.isalpha()]\n",
    "print(only_words[:100])\n",
    "len(only_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "### Removing the stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['david',\n",
       " 'widger',\n",
       " 'war',\n",
       " 'and',\n",
       " 'peace',\n",
       " 'by',\n",
       " 'leo',\n",
       " 'contents',\n",
       " 'book',\n",
       " 'one']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Changing the Case of all words to Lower\n",
    "lower_only_words = [word.lower() for word in only_words]\n",
    "lower_only_words[90:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267919"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering the words & removing the Stop words\n",
    "words_nostopwords = [word for word in lower_only_words if word not in stop_words]\n",
    "len(words_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_nostopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-11d5c07e796a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_nostopwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words_nostopwords' is not defined"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for w in words_nostopwords[20:30]:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "sexi\n",
      "sexiest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "print(PorterStemmer().stem(\"sex\"))\n",
    "print(PorterStemmer().stem(\"sexy\"))\n",
    "print(PorterStemmer().stem(\"sexiest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project\n",
      "gutenberg\n",
      "licens\n",
      "includ\n",
      "ebook\n",
      "onlin\n",
      "titl\n",
      "war\n",
      "peac\n",
      "author\n"
     ]
    }
   ],
   "source": [
    "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "for w in words_nostopwords[20:30]:\n",
    "    print(sno.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "sexy\n",
      "sexiest\n",
      "rock\n",
      "python\n",
      "better\n",
      "best\n",
      "list items\n",
      "item\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"sex\"))\n",
    "print(lemmatizer.lemmatize(\"sexy\"))\n",
    "print(lemmatizer.lemmatize(\"sexiest\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"best\"))\n",
    "print(lemmatizer.lemmatize(\"list items\"))\n",
    "print(lemmatizer.lemmatize(\"items\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj apple\n",
      "is VERB aux be\n",
      "looking VERB ROOT look\n",
      "at ADP prep at\n",
      "buying_U.K. NOUN compound buying_u.k.\n",
      "startup ADJ pobj startup\n",
      "for ADP prep for\n",
      "$ SYM quantmod $\n",
      "1 NUM compound 1\n",
      "Billion NUM nummod billion\n",
      "lastitems NOUN compound lastitem\n",
      "Lastitem PROPN compound lastitem\n",
      "iTem ADJ compound item\n",
      "items NOUN pobj item\n",
      "4.0 NUM npadvmod 4.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "ex = u'Apple is looking at buying_U.K. startup for $1 Billion lastitems Lastitem iTem items 4.0'\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(ex)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Import the stop word list\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(raw_text):        \n",
    "    words = raw_text.lower().split(\" \")                            \n",
    "    words = [w for w in words if len(w)>2]\n",
    "    lemma = WordNetLemmatizer()\n",
    "    lemit_words = [lemma.lemmatize(w) for w in words]\n",
    "    return( \" \".join(lemit_words)) # Join the words back into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple looking buying_u.k. startup for billion lastitems lastitem item item 4.0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(raw_doc):\n",
    "    clean_doc = nlp(raw_doc)\n",
    "    lemma_words = [token.lemma_ for token in clean_doc]\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    words = [w for w in lemma_words if len(w)>2]\n",
    "    return( \" \".join(words)) # Join the words back into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple look buying_u.k. startup billion lastitem lastitem item item 4.0'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
